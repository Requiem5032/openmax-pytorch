{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from utils.graph import draw_confusion_matrix\n",
    "from utils.configs import config\n",
    "from utils.data_loader import get_test_loader\n",
    "from utils.model_list import xception\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from openmax.openmax import compute_openmax\n",
    "from openmax.openmax_utils import (\n",
    "    get_openmax_predict_int,\n",
    "    get_openmax_predict_bin,\n",
    "    get_int_labels,\n",
    "    get_bin_labels,\n",
    "    compute_roc,\n",
    "    compute_pr,\n",
    "    plot_roc,\n",
    "    plot_pr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = xception(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(f'model_weights/fold_{config.fold}/model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.to(device)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl, class_map = get_test_loader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in class_map.items():\n",
    "    class_map[k] = v.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(16, len(class_map)):\n",
    "    class_map.pop(k, None)\n",
    "\n",
    "class_map[16] = 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openmax = []\n",
    "prob_u = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dl:\n",
    "        x = data[0].to(device)\n",
    "        labels = data[1].cpu().numpy()\n",
    "        out = net(x).cpu().numpy()\n",
    "\n",
    "        for logits, label in zip(out, labels):\n",
    "            temp_openmax, temp_prob_u = compute_openmax(logits, config.fold)\n",
    "            openmax.append(temp_openmax)\n",
    "            prob_u.append(temp_prob_u)\n",
    "            y_true.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.asarray(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_bin = get_bin_labels(y_true)\n",
    "y_true_int = get_int_labels(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc = compute_roc(y_true_bin, prob_u)\n",
    "pr = compute_pr(y_true_bin, prob_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_thresholds = roc['thresholds']\n",
    "pr_thresholds = pr['thresholds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = np.argmax(roc['tpr'] - roc['fpr'])\n",
    "best_thresh = roc_thresholds[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_idx = np.argmax(pr['f1'])\n",
    "# best_thresh = pr_thresholds[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique(arr):\n",
    "    unique, counts = np.unique(arr, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bin = []\n",
    "for probs in openmax:\n",
    "    temp = get_openmax_predict_bin(probs, threshold=best_thresh)\n",
    "    y_pred_bin.append(temp)\n",
    "y_pred_bin = np.asarray(y_pred_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_bin = f1_score(y_true_bin, y_pred_bin)\n",
    "pre_bin = precision_score(y_true_bin, y_pred_bin)\n",
    "rec_bin = recall_score(y_true_bin, y_pred_bin)\n",
    "acc_bin = accuracy_score(y_true_bin, y_pred_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = y_true_bin - y_pred_bin\n",
    "get_unique(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique(y_true_bin)\n",
    "get_unique(y_pred_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_int = []\n",
    "for probs in openmax:\n",
    "    temp = get_openmax_predict_int(probs, threshold=best_thresh)\n",
    "    y_pred_int.append(temp)\n",
    "y_pred_int = np.asarray(y_pred_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique(y_true_int)\n",
    "get_unique(y_pred_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true_int, y_pred_int, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(cm, index=class_map.values(), columns=class_map.values())\n",
    "cm_df.to_csv(f'results/cm_fold_{config.fold}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = np.round(cm, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_confusion_matrix(cm, class_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_int_macro = f1_score(y_true_int, y_pred_int, average='macro')\n",
    "pre_int_macro = precision_score(y_true_int, y_pred_int, average='macro')\n",
    "rec_int_macro = recall_score(y_true_int, y_pred_int, average='macro')\n",
    "\n",
    "f1_int_micro = f1_score(y_true_int, y_pred_int, average='micro')\n",
    "pre_int_micro = precision_score(y_true_int, y_pred_int, average='micro')\n",
    "rec_int_micro = recall_score(y_true_int, y_pred_int, average='micro')\n",
    "\n",
    "acc_int = accuracy_score(y_true_int, y_pred_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "metrics['auroc'] = roc['auroc']\n",
    "metrics['aupr'] = pr['aupr']\n",
    "\n",
    "metrics['f1_bin'] = f1_bin\n",
    "metrics['pre_bin'] = pre_bin\n",
    "metrics['rec_bin'] = rec_bin\n",
    "metrics['acc_bin'] = acc_bin\n",
    "\n",
    "metrics['f1_int_macro'] = f1_int_macro\n",
    "metrics['pre_int_macro'] = pre_int_macro\n",
    "metrics['rec_int_macro'] = rec_int_macro\n",
    "\n",
    "metrics['f1_int_micro'] = f1_int_micro\n",
    "metrics['pre_int_micro'] = pre_int_micro\n",
    "metrics['rec_int_micro'] = rec_int_micro\n",
    "\n",
    "metrics['acc_int'] = acc_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results/metrics_{config.fold}.txt', 'w') as file:\n",
    "    for k, v in metrics.items():\n",
    "        file.write(f'{k}: {v}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
